{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f529c306",
   "metadata": {},
   "source": [
    "# Simple GAN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f40048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70fd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load MNIST\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = (x_train.astype('float32') - 127.5) / 127.5\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(256, input_dim=100),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(28*28*1, activation='tanh'),\n",
    "        layers.Reshape((28,28,1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28,28,1)),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "z = layers.Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(img)\n",
    "\n",
    "gan = tf.keras.Model(z, validity)\n",
    "gan.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n",
    "    loss='binary_crossentropy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3567be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "sample_interval = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    real_imgs = x_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    fake_imgs = generator.predict(noise, verbose=0)\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    if epoch % sample_interval == 0:\n",
    "        print(f\"Epoch {epoch} | D loss: {0.5*np.add(d_loss_real, d_loss_fake)} | G loss: {g_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
